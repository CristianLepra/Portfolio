{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo\n",
    "\n",
    "Detección de Neumonía utilizando CNN y herramientas de visualización como GradCam para resaltar areas de la imagen.\n",
    "Utilizando dicha herramienta, al igual que con soluciones como `Saliency map`, podremos destacar las secciones donde la última capa de la CNN pondera al momento de definir una clasificación al input.\n",
    "\n",
    "Ayudan a comprender qué parte o región de cuanta influencia tiene la imagen en la predicción del clase de esa imagen. Los mapas de calor se utilizan para resaltar la píxeles que tienen mayor efecto en la clase de esa imagen.\n",
    "\n",
    "Las capas de CNN se comportan como un modelo no supervisado en este caso. Y nos salvamos del tedioso trabajo de anotar el enorme conjunto de datos con cuadros delimitadores manualmente. La implementación de la técnica del mapa de activación de clases depende de las capas de agrupación promedio global que son aumentado después de la capa convolucional final a espacialmente disminuir las dimensiones de la imagen y reducir los parámetros por lo tanto minimizando el sobreajuste.\n",
    "\n",
    "\n",
    "Grad-CAM utiliza características específicas de la clase para producir mapas de localización de las regiones significativas de la imagen, haciendo que los modelos de caja negra sean más transparentes, al mostrar  visualizaciones que respaldan las predicciones de salida. \n",
    "\n",
    "En otras palabras, Grad-CAM combina gradiente de espacio de píxeles visualización con propiedad discriminativa de clase.\n",
    "\n",
    "\n",
    "GradCAM se utiliza generalmente para resaltar las regiones más importantes desde el punto de vista del modelo CNN que se utilizan para realizar la clasificación.\n",
    "\n",
    "\n",
    "![](https://www.researchgate.net/profile/Elias-Ennadifi/publication/345429793/figure/fig4/AS:1030530145464323@1622708941122/Localization-of-symptoms-using-GradCAM-visualization-method-Left-input-images-Middle.ppm)\n",
    "\n",
    "\n",
    "## Cómo funciona?\n",
    "\n",
    "![](https://www.researchgate.net/profile/Elias-Ennadifi/publication/345429793/figure/fig2/AS:1030530141274114@1622708940983/Flowchart-of-the-proposed-approach.ppm)\n",
    "\n",
    "\n",
    ">NOTE: Para el siguiente Ejemplo se recomienda ejecutar en Kaggle y hacer referencia al dataset `chest-xray-pneumonia`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalamos los addons de tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow-addons\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "EPOCHS = 30\n",
    "IM_SIZE = 512\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "tf.random.set_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if \"GPU\" not in device_name:\n",
    "    print(\"GPU device not found\")\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploramos los directorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('../input/chest-xray-pneumonia/chest_xray/chest_xray/'):\n",
    "    print(dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificamos algunos filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filenames = tf.io.gfile.glob('../input/chest-xray-pneumonia/chest_xray/chest_xray/train/*/*') # Utilizar el path correspondiente\n",
    "# filenames.extend(tf.io.gfile.glob('../input/chest-xray-pneumonia/chest_xray/chest_xray/val/*/*'))\n",
    "filenames.extend(tf.io.gfile.glob('../input/chest-xray-pneumonia/chest_xray/chest_xray/test/*/*'))\n",
    "\n",
    "filenames[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armamos un Dataframe a partir de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "for el in range(0, len(filenames)):\n",
    "    target = filenames[el].split('/')[-2]\n",
    "    path = filenames[el]\n",
    "    data.loc[el, 'filename'] = path\n",
    "    data.loc[el, 'class'] = target\n",
    "\n",
    "print(data['class'].value_counts(dropna=False))\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = shuffle(data, random_state=42)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creamos una función que elimine todo lo que no sea imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indexes=[]\n",
    "\n",
    "def func(x):\n",
    "    if x[-5:] != '.jpeg':\n",
    "        idx = data[data['filename'] == x].index\n",
    "        indexes.append(idx[0])\n",
    "        print(idx[0], x)\n",
    "    return x\n",
    "\n",
    "data['filename'].map(func)\n",
    "\n",
    "print(data.shape)\n",
    "data.drop(index=indexes, axis=0, inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizamos el split `train_data`, `test_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.1, random_state=42, stratify=data['class'])\n",
    "test_data = test_data[ : test_data.shape[0] // BATCH_SIZE * BATCH_SIZE]\n",
    "print(train_data['class'].value_counts(dropna=False))\n",
    "print(test_data['class'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizamos el split `train_data`, `val_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42, stratify=train_data['class'])\n",
    "print(train_data['class'].value_counts(dropna=False))\n",
    "print(val_data['class'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definimos un `ImageDataGenerator` y `Augmentation` para expandir nuestro dataset (Solo para training!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                             zoom_range=0.1, # 0.05\n",
    "                             brightness_range=[0.9, 1.0],\n",
    "                             height_shift_range=0.05, \n",
    "                             width_shift_range=0.05,\n",
    "                             rotation_range=10, \n",
    "                            )\n",
    "\n",
    "\n",
    "\n",
    "train_gen = datagen.flow_from_dataframe(train_data,\n",
    "                                        x_col=\"filename\",\n",
    "                                        y_col=\"class\",\n",
    "                                        target_size=(IM_SIZE, IM_SIZE),\n",
    "                                        color_mode='grayscale',\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        class_mode='categorical',\n",
    "                                        shuffle=True,\n",
    "                                        num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "val_gen = test_datagen.flow_from_dataframe(val_data,\n",
    "                                        x_col=\"filename\",\n",
    "                                        y_col=\"class\",\n",
    "                                        target_size=(IM_SIZE, IM_SIZE),\n",
    "                                        color_mode='grayscale',\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        class_mode='categorical',\n",
    "                                        shuffle=False,\n",
    "                                        num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "test_gen = test_datagen.flow_from_dataframe(test_data,\n",
    "                                        x_col=\"filename\",\n",
    "                                        y_col=\"class\",\n",
    "                                        target_size=(IM_SIZE, IM_SIZE),\n",
    "                                        color_mode='grayscale',\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        class_mode='categorical',\n",
    "                                        shuffle=False,\n",
    "                                        num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definimos nuestra arquitectura CNN (Se puede optar por una red pre-entrenada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define CNN model\n",
    "def create_model():\n",
    "    with tf.device('/gpu:0'):        \n",
    "    \n",
    "        # Model input\n",
    "        input_layer = layers.Input(shape=(IM_SIZE, IM_SIZE, 1), name='input')  \n",
    "        \n",
    "        # First block\n",
    "        x = layers.Conv2D(filters=128, kernel_size=3, \n",
    "                          activation='relu', padding='same', \n",
    "                          name='conv2d_1')(input_layer)\n",
    "        x = layers.MaxPool2D(pool_size=2, name='maxpool2d_1')(x)\n",
    "        x = layers.Dropout(0.1, name='dropout_1')(x)\n",
    "\n",
    "        # Second block\n",
    "        x = layers.Conv2D(filters=128, kernel_size=3, \n",
    "                          activation='relu', padding='same', \n",
    "                          name='conv2d_2')(x)\n",
    "        x = layers.MaxPool2D(pool_size=2, name='maxpool2d_2')(x)\n",
    "        x = layers.Dropout(0.1, name='dropout_2')(x)\n",
    "\n",
    "        # Third block\n",
    "        x = layers.Conv2D(filters=128, kernel_size=3, \n",
    "                          activation='relu', padding='same', \n",
    "                          name='conv2d_3')(x)\n",
    "        x = layers.MaxPool2D(pool_size=2, name='maxpool2d_3')(x)\n",
    "        x = layers.Dropout(0.1, name='dropout_3')(x)\n",
    "\n",
    "        # Fourth block\n",
    "        x = layers.Conv2D(filters=256, kernel_size=3, \n",
    "                          activation='relu', padding='same', \n",
    "                          name='conv2d_4')(x)\n",
    "        x = layers.MaxPool2D(pool_size=2, name='maxpool2d_4')(x)\n",
    "        x = layers.Dropout(0.1, name='dropout_4')(x)\n",
    "\n",
    "        # Fifth block\n",
    "        x = layers.Conv2D(filters=256, kernel_size=3, \n",
    "                          activation='relu', padding='same', \n",
    "                          name='conv2d_5')(x)\n",
    "        x = layers.MaxPool2D(pool_size=2, name='maxpool2d_5')(x)\n",
    "        x = layers.Dropout(0.1, name='dropout_5')(x)\n",
    "\n",
    "        # Sixth block\n",
    "        x = layers.Conv2D(filters=512, kernel_size=3, \n",
    "                          activation='relu', padding='same', \n",
    "                          name='conv2d_6')(x)\n",
    "        x = layers.MaxPool2D(pool_size=2, name='maxpool2d_6')(x)\n",
    "        x = layers.Dropout(0.1, name='dropout_6')(x)\n",
    "\n",
    "        # Seventh block\n",
    "        x = layers.Conv2D(filters=512, kernel_size=3, \n",
    "                          activation='relu', padding='same', \n",
    "                          name='conv2d_7')(x)\n",
    "        x = layers.MaxPool2D(pool_size=2, name='maxpool2d_7')(x)\n",
    "        x = layers.Dropout(0.1, name='dropout_7')(x)\n",
    "        \n",
    "        # GlobalAveragePooling\n",
    "        x = layers.GlobalAveragePooling2D(name='global_average_pooling2d')(x)   \n",
    "        x = layers.Flatten()(x)\n",
    "        \n",
    "        # Head\n",
    "        x = layers.Dense(1024,activation='relu')(x)\n",
    "        x = layers.Dropout(0.1, name='dropout_head_2')(x)\n",
    "        x = layers.Dense(128,activation='relu')(x)\n",
    "        \n",
    "        # Output\n",
    "        output = layers.Dense(units=2, \n",
    "                              activation='softmax', \n",
    "                              name='output')(x)\n",
    "\n",
    "\n",
    "        model = Model(input_layer, output)   \n",
    "\n",
    "        F_1_macro = [tfa.metrics.f_scores.F1Score(num_classes=2, average=\"macro\")] \n",
    "        \n",
    "        model.compile(optimizer='adam', \n",
    "                      loss='categorical_crossentropy', \n",
    "                      metrics=F_1_macro)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feed_data(dataset):\n",
    "    return dataset.prefetch(buffer_size=AUTOTUNE)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_time = datetime.datetime.now()\n",
    "\n",
    "\n",
    "train_steps = train_gen.samples // BATCH_SIZE\n",
    "valid_steps = val_gen.samples // BATCH_SIZE\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=8, mode=\"min\")\n",
    "checkpoint = ModelCheckpoint(\"acc-{val_loss:.4f}.h5\", monitor=\"val_loss\", verbose=0, \n",
    "                             save_best_only=True, save_weights_only=True, mode=\"min\")\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=3, \n",
    "                                            min_lr=1e-7, verbose=1, mode=\"min\")\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_steps=valid_steps,\n",
    "    callbacks=[\n",
    "                checkpoint, \n",
    "                early_stopping, \n",
    "                learning_rate_reduction],\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "requared_time = datetime.datetime.now() - init_time\n",
    "print(f'\\nRequired time:  {str(requared_time)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[0:, ['loss', 'val_loss']].plot()\n",
    "print(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_steps = test_gen.samples // BATCH_SIZE\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_gen, steps=test_steps)\n",
    "print('\\naccuracy:', test_acc, 'loss: ',test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict = model.predict(test_gen, steps=test_steps)\n",
    "y_hat = np.argmax(predict, axis=1)\n",
    "y_hat[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_labels_df = pd.DataFrame()\n",
    "test_labels_df[['class']] = test_data[['class']]\n",
    "test_labels_df['class'] = test_labels_df['class'].map({'NORMAL':0, 'PNEUMONIA':1})\n",
    "\n",
    "y_test = np.array(test_labels_df['class'])\n",
    "y_test[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_hat), '\\n')\n",
    "cm = confusion_matrix(y_test, y_hat)\n",
    "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt='.0f', cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "model.save('/kaggle/working/model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "!pip install tf_keras_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_keras_vis \n",
    "from matplotlib import cm\n",
    "tf_keras_vis.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_keras_vis.gradcam import Gradcam\n",
    "from tf_keras_vis.utils.scores import CategoricalScore\n",
    "from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
    "from tf_keras_vis.gradcam_plus_plus import GradcamPlusPlus\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def model_modifier_function(cloned_model):\n",
    "    '''modify model activation'''\n",
    "    cloned_model.layers[-1].activation = tf.keras.activations.linear\n",
    "\n",
    "\n",
    "def get_gradcam_plus(img,\n",
    "                    score,\n",
    "                    model=model,\n",
    "                    model_modifier=ReplaceToLinear()):    \n",
    "    gradcampls = GradcamPlusPlus(model,\n",
    "                          model_modifier=model_modifier,\n",
    "                          clone=True)\n",
    "    heatmap = gradcampls(score, img)\n",
    "    heatmap = np.uint8(cm.jet(heatmap[0])[..., :3] * 255)\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(test_gen)\n",
    "iterator.next()\n",
    "iterator.next()\n",
    "\n",
    "imgs,labs = iterator.next()\n",
    "real_labs = list(np.argmax(labs, axis=1))\n",
    "print(real_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {0: 'NORMAL', \n",
    "              1: 'PNEUMONIA',} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = '20'\n",
    "plt.subplots(8,2,figsize=(20,100))\n",
    "\n",
    "idx=1\n",
    "for i,img in enumerate(imgs[:8]):\n",
    "#     print(f'Image {i}')    \n",
    "    img_4d = tf.cast(tf.reshape(img, [1, IM_SIZE, IM_SIZE, 1]), tf.float32)\n",
    "    predict = model.predict(img_4d)\n",
    "    prd = np.argmax(predict)\n",
    "#     print(f'class: {class_dict[prd]}')\n",
    "    score1 = CategoricalScore(prd)\n",
    "    original_lab = real_labs[i]\n",
    "    \n",
    "    plt.subplot(8,2,idx)\n",
    "    plt.title(f'orignal {class_dict[original_lab]}')\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "    idx+=1\n",
    "    \n",
    "    plt.subplot(8,2,idx)\n",
    "    gdcam_pls = get_gradcam_plus(img, score1)\n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "    if prd:\n",
    "        plt.imshow(gdcam_pls, alpha=0.2, cmap='jet')\n",
    "        proba = round(float(predict[0][1]), 4)\n",
    "    else:\n",
    "        proba = round(float(predict[0][0]), 4)\n",
    "    plt.title(f'predicted {class_dict[prd]}  {proba} probability')\n",
    "    plt.axis('off')\n",
    "    idx+=1\n",
    "    if idx>20:\n",
    "        break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source: https://www.kaggle.com/code/artyomkolas/pneumonia-cnn-feature-visualization\n",
    "- Updated and tweaked by Alejandro Casas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
