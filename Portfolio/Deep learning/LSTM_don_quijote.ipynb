{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Uso de LSTM para generar secuencias de texto: DON QUIJOTE DE LA MANCHA**","metadata":{"id":"rlXbJ2FSuA2o"}},{"cell_type":"markdown","source":"- Modelos de lenguaje, dada una serie de palabras, predecir (probabilistico) la proxima palabra o secuencia\n- Sampling estocástico (elegir random en base a la probabilidad), cuyas ventajas son:\n     - Produce frases más interesantes (creatividad)\n     - Parámetro que controla esta estocasticidad es temperatura\n","metadata":{"id":"sNvVcaQZNOcU"}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.preprocessing import sequence\nimport numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Conv1D\nfrom tensorflow.keras.layers import MaxPooling1D\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import Dropout","metadata":{"execution":{"iopub.status.busy":"2023-11-02T00:33:05.660366Z","iopub.execute_input":"2023-11-02T00:33:05.660751Z","iopub.status.idle":"2023-11-02T00:33:13.280891Z","shell.execute_reply.started":"2023-11-02T00:33:05.660720Z","shell.execute_reply":"2023-11-02T00:33:13.280081Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Cargamos texto del Quijote\nwith open(\"/kaggle/input/25agostocuentos/veinticinco_agosto_cuentos.txt\",'r') as f:\n    replace_chars = {'\\n': ' ',\n                    '\\xad':'',\n                    '±':'',\n                    '³':'',\n                    'º':'',\n                    '¼':'',\n                    'â':'a',\n                    'ã':'a',\n                    '©':''\n    }\n    corpus = f.read().lower()\n    for old, new in replace_chars.items():\n        corpus = corpus.replace(old,new)\n\n# Vectorizar el texto\n\nmaxlen = 60 #longitud de las secuencias\nstep = 3 # cada cuantas letras empezar una secuencia\n\nsentences = []\nnext_chars = []\n\nfor i in range(0,len(corpus) - maxlen,step):\n    sentences.append(corpus[i: i + maxlen])\n    next_chars.append(corpus[i + maxlen])","metadata":{"id":"MWDKGvW0NOcg","execution":{"iopub.status.busy":"2023-11-02T00:33:41.662416Z","iopub.execute_input":"2023-11-02T00:33:41.662839Z","iopub.status.idle":"2023-11-02T00:33:41.714219Z","shell.execute_reply.started":"2023-11-02T00:33:41.662808Z","shell.execute_reply":"2023-11-02T00:33:41.713521Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Vamos a crear nuestro corpus de letras\nunique_chars = sorted(list(set(corpus)))\nchar_indices = {char : i for i,char in enumerate(unique_chars) }\nprint(char_indices)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ek1hsL3qNOcj","outputId":"f4149953-d833-4893-d7bc-68997f087cab","execution":{"iopub.status.busy":"2023-11-02T00:33:41.715923Z","iopub.execute_input":"2023-11-02T00:33:41.716198Z","iopub.status.idle":"2023-11-02T00:33:41.724359Z","shell.execute_reply.started":"2023-11-02T00:33:41.716174Z","shell.execute_reply":"2023-11-02T00:33:41.723432Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"{' ': 0, '!': 1, '(': 2, ')': 3, ',': 4, '-': 5, '.': 6, '/': 7, '0': 8, '1': 9, '2': 10, '3': 11, '4': 12, '5': 13, '6': 14, '7': 15, '8': 16, '9': 17, ':': 18, ';': 19, '<': 20, '?': 21, '[': 22, ']': 23, 'a': 24, 'b': 25, 'c': 26, 'd': 27, 'e': 28, 'f': 29, 'g': 30, 'h': 31, 'i': 32, 'j': 33, 'k': 34, 'l': 35, 'm': 36, 'n': 37, 'o': 38, 'p': 39, 'q': 40, 'r': 41, 's': 42, 't': 43, 'u': 44, 'v': 45, 'w': 46, 'x': 47, 'y': 48, 'z': 49, '¡': 50, 'ª': 51, '«': 52, '»': 53, '¿': 54, 'á': 55, 'ä': 56, 'ç': 57, 'é': 58, 'ê': 59, 'í': 60, 'ñ': 61, 'ó': 62, 'ô': 63, 'ú': 64, 'ü': 65, '—': 66, '’': 67, '“': 68, '”': 69, '…': 70}\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n\n# Vectorizacion (one hot encoding)\nx = np.zeros((len(sentences), maxlen,len(unique_chars)), dtype=np.bool) # cada secuencia, hot encoded\ny = np.zeros((len(sentences), len(unique_chars)), dtype=np.bool) # para cada secuencia, el siguiente caracter hot encoded\nfor i, sentence in enumerate(sentences):\n    for t, char in enumerate(sentence):\n        x[i,t,char_indices[char]] = 1\n    y[i,char_indices[next_chars[i]]] = 1\nprint(x.shape)\nprint(y.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"teYVBpDcNOcp","outputId":"d3e840a5-b509-499e-dae2-8d6772a57722","execution":{"iopub.status.busy":"2023-11-02T00:33:41.725618Z","iopub.execute_input":"2023-11-02T00:33:41.726014Z","iopub.status.idle":"2023-11-02T00:33:43.456798Z","shell.execute_reply.started":"2023-11-02T00:33:41.725983Z","shell.execute_reply":"2023-11-02T00:33:43.455866Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_32/1896659240.py:4: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  x = np.zeros((len(sentences), maxlen,len(unique_chars)), dtype=np.bool) # cada secuencia, hot encoded\n/tmp/ipykernel_32/1896659240.py:5: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  y = np.zeros((len(sentences), len(unique_chars)), dtype=np.bool) # para cada secuencia, el siguiente caracter hot encoded\n","output_type":"stream"},{"name":"stdout","text":"(52873, 60, 71)\n(52873, 71)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Modelo con LSTM\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\n\n# Cuando se concatenan LSTM, parametro return_sequences=True excepto en la ultima\n# Las capas intermedias pasan toda la secuencia de outputs, pero la ultima solo pasa el ultimo output\nmodel = models.Sequential()\nmodel.add(LSTM(256, input_shape=(maxlen,len(unique_chars)),return_sequences=True)) # devuelve una secuencia de vectores de 128 dimensiones\nmodel.add(LSTM(64))\nmodel.add(layers.Dense(len(unique_chars), activation='softmax')) # softmax para que el output sume 1\n\nmodel.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['acc'])\nmodel.summary()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pvXbRmkDNOcu","outputId":"10e797fb-3887-4951-b487-2d474669452a","execution":{"iopub.status.busy":"2023-11-02T00:33:43.458513Z","iopub.execute_input":"2023-11-02T00:33:43.458903Z","iopub.status.idle":"2023-11-02T00:33:47.066054Z","shell.execute_reply.started":"2023-11-02T00:33:43.458868Z","shell.execute_reply":"2023-11-02T00:33:47.065160Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm (LSTM)                 (None, 60, 256)           335872    \n                                                                 \n lstm_1 (LSTM)               (None, 64)                82176     \n                                                                 \n dense (Dense)               (None, 71)                4615      \n                                                                 \n=================================================================\nTotal params: 422,663\nTrainable params: 422,663\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# Entrenar el modelo\nhistory = model.fit(x,y,batch_size=128,epochs=30)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q_bQWba9NOcy","outputId":"2e18cd71-8603-40bb-8e47-4544cfaac5c7","execution":{"iopub.status.busy":"2023-11-02T00:33:47.068010Z","iopub.execute_input":"2023-11-02T00:33:47.068292Z","iopub.status.idle":"2023-11-02T00:37:12.427098Z","shell.execute_reply.started":"2023-11-02T00:33:47.068268Z","shell.execute_reply":"2023-11-02T00:37:12.426272Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Epoch 1/30\n414/414 [==============================] - 12s 14ms/step - loss: 3.0295 - acc: 0.1752\nEpoch 2/30\n414/414 [==============================] - 6s 14ms/step - loss: 2.8853 - acc: 0.2163\nEpoch 3/30\n414/414 [==============================] - 6s 14ms/step - loss: 2.5778 - acc: 0.2635\nEpoch 4/30\n414/414 [==============================] - 6s 14ms/step - loss: 2.4266 - acc: 0.2888\nEpoch 5/30\n414/414 [==============================] - 6s 14ms/step - loss: 2.3339 - acc: 0.3149\nEpoch 6/30\n414/414 [==============================] - 6s 14ms/step - loss: 2.2617 - acc: 0.3326\nEpoch 7/30\n414/414 [==============================] - 6s 14ms/step - loss: 2.2066 - acc: 0.3489\nEpoch 8/30\n414/414 [==============================] - 6s 14ms/step - loss: 2.1607 - acc: 0.3584\nEpoch 9/30\n414/414 [==============================] - 6s 14ms/step - loss: 2.1214 - acc: 0.3673\nEpoch 10/30\n414/414 [==============================] - 6s 14ms/step - loss: 2.0842 - acc: 0.3772\nEpoch 11/30\n414/414 [==============================] - 6s 14ms/step - loss: 2.0491 - acc: 0.3858\nEpoch 12/30\n414/414 [==============================] - 6s 14ms/step - loss: 2.0144 - acc: 0.3968\nEpoch 13/30\n414/414 [==============================] - 6s 14ms/step - loss: 1.9792 - acc: 0.4098\nEpoch 14/30\n414/414 [==============================] - 6s 14ms/step - loss: 1.9430 - acc: 0.4207\nEpoch 15/30\n414/414 [==============================] - 6s 14ms/step - loss: 1.9077 - acc: 0.4321\nEpoch 16/30\n414/414 [==============================] - 6s 14ms/step - loss: 1.8712 - acc: 0.4422\nEpoch 17/30\n414/414 [==============================] - 6s 14ms/step - loss: 1.8343 - acc: 0.4535\nEpoch 18/30\n414/414 [==============================] - 6s 14ms/step - loss: 1.7940 - acc: 0.4657\nEpoch 19/30\n414/414 [==============================] - 6s 14ms/step - loss: 1.7544 - acc: 0.4766\nEpoch 20/30\n414/414 [==============================] - 6s 14ms/step - loss: 1.7131 - acc: 0.4886\nEpoch 21/30\n414/414 [==============================] - 6s 14ms/step - loss: 1.6680 - acc: 0.5025\nEpoch 22/30\n414/414 [==============================] - 6s 14ms/step - loss: 1.6214 - acc: 0.5163\nEpoch 23/30\n414/414 [==============================] - 6s 14ms/step - loss: 1.5710 - acc: 0.5316\nEpoch 24/30\n414/414 [==============================] - 6s 14ms/step - loss: 1.5180 - acc: 0.5477\nEpoch 25/30\n414/414 [==============================] - 6s 14ms/step - loss: 1.4625 - acc: 0.5623\nEpoch 26/30\n414/414 [==============================] - 6s 14ms/step - loss: 1.4072 - acc: 0.5797\nEpoch 27/30\n414/414 [==============================] - 6s 14ms/step - loss: 1.3489 - acc: 0.5966\nEpoch 28/30\n414/414 [==============================] - 6s 14ms/step - loss: 1.2881 - acc: 0.6173\nEpoch 29/30\n414/414 [==============================] - 6s 14ms/step - loss: 1.2268 - acc: 0.6366\nEpoch 30/30\n414/414 [==============================] - 6s 14ms/step - loss: 1.1674 - acc: 0.6541\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n\ndef transform_distribution(predictions, temperature=0.5):\n    predictions = np.array(predictions).astype('float64')\n    # temperaturas altas: mas entropia (mas aleatorio)\n    # temperaturas bajas: menos estocasticidad (mas deterministico)\n    predictions = np.log(predictions) / temperature\n    exp_predictions = np.exp(predictions)\n    predictions = exp_predictions / np.sum(exp_predictions) # asegurarse que los valores suman 1 (probabilidad)\n    probs = np.random.multinomial(1, predictions, 1)\n    return np.argmax(probs)","metadata":{"id":"kWmOJsaCNOcW","execution":{"iopub.status.busy":"2023-11-02T00:37:12.428298Z","iopub.execute_input":"2023-11-02T00:37:12.428578Z","iopub.status.idle":"2023-11-02T00:37:12.434764Z","shell.execute_reply.started":"2023-11-02T00:37:12.428553Z","shell.execute_reply":"2023-11-02T00:37:12.433928Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import random\n# generar secuencias arbitrarias de texto\npredict_length = 400\ntemperature = 0.5   #MIS NOTAS: bajarle este valor para que sea menos creativo\n# random text seed\nstart_index = random.randint(0,len(corpus) - maxlen - 1)\ninput_text = corpus[start_index: start_index + maxlen]\nprint('Seed: ' + input_text)\ngenerated_text = input_text\n\nfor i in range(predict_length):\n    sampled = np.zeros((1, maxlen, len(unique_chars)))\n    for t, char in enumerate(input_text):\n        sampled[0,t,char_indices[char]] = 1.\n\n    prediction = model.predict(sampled, verbose=0)[0]\n    next_index = transform_distribution(prediction,temperature)\n    next_char = unique_chars[next_index]\n\n    #pegar el nuevo texto\n    input_text += next_char\n    generated_text += next_char\n    input_text = input_text[1:]\n\ngenerated_text","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":88},"id":"KW3cBGtFNOc3","outputId":"fb48a0c7-b460-4ade-d7af-a2b5e8c5b35e","tags":[],"execution":{"iopub.status.busy":"2023-11-02T00:40:55.629441Z","iopub.execute_input":"2023-11-02T00:40:55.630306Z","iopub.status.idle":"2023-11-02T00:41:15.734074Z","shell.execute_reply.started":"2023-11-02T00:40:55.630269Z","shell.execute_reply":"2023-11-02T00:41:15.733043Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Seed: r un camino de la llanura. me pregunté sin mucha curiosidad \n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'r un camino de la llanura. me pregunté sin mucha curiosidad de la pespericidad de la peesta.  —en una coma la insimena los deligaciones de la liertat de la labra de la presidia del conve de las palabras.  la con la inferedad de la convela y de la peleración del ispertirio de la persada mentada a se losor y la pendía es para del congrena o por la persado de la persa de la constanada de cada estrabaran de la premida por la convercidad de la peesta.  —no ente'"},"metadata":{}}]}]}