# -*- coding: utf-8 -*-
"""15.ejercicio_integrador_cnn_Colabresuelto

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_0D7G-DW3ntKQHhMZ556Zjmq7I3tImKL

# Ejercicio - CNN

Tomando el dataset de dígitos del MNIST, construir una red convolusional, extraer su reporte de métricas, e integrar con [Neptune](https://app.neptune.ai).
"""

#!pip install neptune
#!pip install neptune-tensorflow-keras

# Neptune
import neptune
from neptune.integrations.tensorflow_keras import NeptuneCallback
from neptune.types import File

run = neptune.init_run(
    project="cristianleprab/Proyecto-", # your project
    api_token="eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJiNWUzMTg0OC00MjJiLTRiY2EtOWMzNy04NzJlYmFiMzE4OGUifQ==",)  # your credentials

model_version = neptune.init_model_version(
    model="PROY-MOD",
    project="cristianleprab/Proyecto-",
    api_token="eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJiNWUzMTg0OC00MjJiLTRiY2EtOWMzNy04NzJlYmFiMzE4OGUifQ==", # your credentials
)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.datasets import mnist
from keras.callbacks import EarlyStopping, ModelCheckpoint
import warnings
warnings.filterwarnings("ignore")

# Defining an Early Stopping and Model Checkpoints
early_stopping = EarlyStopping(monitor='val_loss',
                               patience=5,
                               restore_best_weights=True) #TODO

checkpoint = ModelCheckpoint(filepath='best_model.h5',
                             monitor='val_accuracy',
                             save_best_only=True,
                             mode='max') #TODO

inputs = keras.Input(shape=(28, 28, 1))
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(inputs)
#TODO
outputs = layers.Dense(10, activation="softmax")(x)
model = keras.Model(inputs=inputs, outputs=outputs)

inputs = keras.Input(shape=(28, 28, 1))
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(inputs)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.Flatten()(x)
outputs = layers.Dense(10, activation="softmax")(x)
model = keras.Model(inputs=inputs, outputs=outputs)

parameters = {
    "dense_units": 10,
    "activation": "relu",
    "batch_size": 64,
    "n_epochs": 50,
}
   #TODO

run["model/parameters"] = parameters #TODO

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images = train_images.reshape((60000, 28, 28, 1))
train_images = train_images.astype("float32") / 255
test_images = test_images.reshape((10000, 28, 28, 1))
test_images = test_images.astype("float32") / 255


model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])


H = model.fit(train_images,
              train_labels,
              epochs=25,
              batch_size=64,
              validation_split=0.2,
              callbacks=[early_stopping, checkpoint]) #TODO callbacks

model.save("best_model.h5")

# Función para salvar las imágenes y CSV en disco

import os

import pandas as pd
from PIL import Image
import tensorflow as tf

def make_dirs(path_list):
    for path in path_list:
        if not os.path.exists(path):
            os.makedirs(path)

def make_containing_dirs(path_list):

    for path in path_list:
        dir_name = os.path.dirname(path)
        if not os.path.exists(dir_name):
            os.makedirs(dir_name)

class SaverMNIST():
    def __init__(self, image_train_path, image_test_path, csv_train_path,
                 csv_test_path):

        self._image_format = '.png'

        self.store_image_paths = [image_train_path, image_test_path]
        self.store_csv_paths = [csv_train_path, csv_test_path]

        make_dirs(self.store_image_paths)
        make_containing_dirs(self.store_csv_paths)

        # Load MNIST dataset
        mnist = tf.keras.datasets.mnist
        self.data = mnist.load_data()

    def run(self):

        for collection, store_image_path, store_csv_path in zip(self.data,
                                                                self.store_image_paths,
                                                                self.store_csv_paths):

            labels_list = []
            paths_list = []

            for index, (image, label) in enumerate(zip(collection[0],
                                                       collection[1])):
                im = Image.fromarray(image)
                width, height = im.size
                image_name = str(index) + self._image_format

                # Build save path
                save_path = os.path.join(store_image_path, image_name)
                im.save(save_path)

                labels_list.append(label)
                paths_list.append(save_path)

            df = pd.DataFrame({'image_paths':paths_list, 'labels': labels_list})

            df.to_csv(store_csv_path)

mnist_saver = SaverMNIST(image_train_path='dataset/images',
                         image_test_path='dataset/images',
                         csv_train_path='dataset/train.csv',
                         csv_test_path='dataset/test.csv')

# Write files into disk
mnist_saver.run()

neptune_callback = NeptuneCallback(run=run, base_namespace='metrics')

test_loss, test_acc = model.evaluate(test_images, test_labels)
print(f"Test accuracy: {test_acc:.3f}")

run["evaluation/accuracy"] = test_acc #TODO
run["evaluation/loss"] = test_loss#TODO

run["train_dataset"].track_files("dataset/train.csv")#TODO
run["test_dataset"].track_files("dataset/test.csv")#TODO

model_version["model"].upload("best_model.h5")#TODO

run.stop()

# Evaluando el modelo de predicción con las imágenes de test
print("[INFO]: Evaluando red neuronal...")
model.predict(test_images)
loss, accuracy = model.evaluate(test_images, test_labels)
print('Loss {}, accuracy {}'.format(loss, accuracy))

# Mostramos gráfica de accuracy y losses
plt.style.use("ggplot")
plt.figure()

# Asegúrate de que las listas tienen la misma longitud
epochs = range(1, len(H.history['loss']) + 1)

plt.plot(epochs, H.history["loss"], label="train_loss")
plt.plot(epochs, H.history["val_loss"], label="val_loss")
plt.plot(epochs, H.history["accuracy"], label="train_acc")
plt.plot(epochs, H.history["val_accuracy"], label="val_acc")

plt.title("Training Loss and Accuracy")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()

import numpy as np
y_pred = model.predict(test_images)[0]
print(np.argmax(y_pred))
plt.imshow(test_images[0].reshape(28,28), cmap='gray', interpolation='none')